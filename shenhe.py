# Spesç›´æ’­é—´å®£ä¼ åˆè§„å®¡æŸ¥Agent
# åŸºäºRAGæŠ€æœ¯çš„æ™ºèƒ½åˆè§„å®¡æŸ¥ç³»ç»Ÿ

import os
import re
import json
import unicodedata
import base64
import requests
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from dotenv import load_dotenv
from PIL import Image
import cv2
import numpy as np

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.tools import tool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain import hub

# æ–‡æ¡£åŠ è½½å™¨
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredFileLoader
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

@dataclass
class ComplianceResult:
    """åˆè§„å®¡æŸ¥ç»“æœæ•°æ®ç±»"""
    category: str
    original_text: str
    review_result: str
    hit_word: str = ""
    risk_category: str = ""
    risk_level: str = ""
    rule_source: str = ""
    brief_description: str = ""
    manual_review_needed: bool = False

class ImageProcessor:
    """å›¾ç‰‡å¤„ç†æ¨¡å— - ä½¿ç”¨ç¡…åŸºæµåŠ¨Qwen/Qwen2.5-VL-32B-Instructæ¨¡å‹"""
    
    def __init__(self):
        # ç¡…åŸºæµåŠ¨APIé…ç½®
        self.api_key = os.getenv('SILICONFLOW_API_KEY')
        self.base_url = os.getenv('SILICONFLOW_BASE_URL', 'https://api.siliconflow.cn/v1')
        self.model = "Qwen/Qwen2.5-VL-32B-Instruct"
        
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ SILICONFLOW_API_KEY")
        
        print(f"âœ… ç¡…åŸºæµåŠ¨å¤šæ¨¡æ€æ¨¡å‹å·²åˆå§‹åŒ–: {self.model}")
    
    def encode_image_to_base64(self, image_path: str) -> str:
        """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64"""
        try:
            with open(image_path, 'rb') as image_file:
                image_data = image_file.read()
                base64_string = base64.b64encode(image_data).decode('utf-8')
                return base64_string
        except Exception as e:
            raise ValueError(f"æ— æ³•è¯»å–å›¾ç‰‡æ–‡ä»¶: {e}")
    
    def extract_text_from_image(self, image_path: str) -> str:
        """ä½¿ç”¨ç¡…åŸºæµåŠ¨å¤šæ¨¡æ€æ¨¡å‹ä»å›¾ç‰‡ä¸­æå–æ–‡å­—"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                raise ValueError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            
            # ç¼–ç å›¾ç‰‡ä¸ºbase64
            base64_image = self.encode_image_to_base64(image_path)
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "è¯·ä»”ç»†è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ã€‚è¦æ±‚ï¼š1. å‡†ç¡®è¯†åˆ«ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç¬¦å·ç­‰æ‰€æœ‰æ–‡å­—ï¼›2. æŒ‰ç…§å›¾ç‰‡ä¸­çš„åŸå§‹å¸ƒå±€é¡ºåºè¾“å‡ºæ–‡å­—ï¼›3. ä¿æŒæ–‡å­—çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼›4. ç‰¹åˆ«å…³æ³¨äº§å“åç§°ã€åŠŸæ•ˆæè¿°ã€å®£ä¼ è¯­ç­‰å…³é”®ä¿¡æ¯ï¼›5. å¦‚æœæ–‡å­—æœ‰ç‰¹æ®Šæ ¼å¼ï¼ˆå¦‚åŠ ç²—ã€é¢œè‰²ç­‰ï¼‰ï¼Œè¯·åœ¨è¾“å‡ºä¸­è¯´æ˜ã€‚è¯·ç›´æ¥è¾“å‡ºè¯†åˆ«åˆ°çš„æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–çš„è§£é‡Šã€‚"
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 2000,
                "temperature": 0.1
            }
            
            # å‘é€è¯·æ±‚
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    extracted_text = result['choices'][0]['message']['content']
                    
                    # æ¸…ç†æ–‡æœ¬
                    extracted_text = extracted_text.strip()
                    extracted_text = re.sub(r'\n+', ' ', extracted_text)  # å°†å¤šä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
                    extracted_text = re.sub(r'\s+', ' ', extracted_text)  # å°†å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
                    
                    print(f"âœ… æˆåŠŸä»å›¾ç‰‡æå–æ–‡å­—: {extracted_text[:100]}...")
                    return extracted_text
                else:
                    print("âŒ APIå“åº”æ ¼å¼é”™è¯¯")
                    return ""
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return ""
                
        except Exception as e:
            print(f"âŒ å›¾ç‰‡æ–‡å­—æå–å¤±è´¥: {e}")
            return ""
    
    def is_image_file(self, file_path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶"""
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp'}
        file_ext = os.path.splitext(file_path)[1].lower()
        return file_ext in image_extensions

class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†æ¨¡å—"""
    
    @staticmethod
    def remove_brackets(text: str) -> str:
        """åˆ é™¤æ–¹æ‹¬å·å­—ç¬¦ï¼Œä¿ç•™æ‹¬å·å†…æ–‡å­—"""
        return re.sub(r'[\[\]]', '', text)
    
    @staticmethod
    def normalize_punctuation(text: str) -> str:
        """å°†å…¨è§’æ ‡ç‚¹ç¬¦å·è½¬æ¢ä¸ºåŠè§’"""
        return unicodedata.normalize('NFKC', text)
    
    @staticmethod
    def to_lowercase(text: str) -> str:
        """å°†ASCIIè‹±æ–‡å­—æ¯è½¬æ¢ä¸ºå°å†™"""
        result = ""
        for char in text:
            if char.isascii() and char.isalpha():
                result += char.lower()
            else:
                result += char
        return result
    
    @staticmethod
    def strip_whitespace(text: str) -> str:
        """å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦"""
        return text.strip()
    
    @staticmethod
    def preprocess(text: str) -> str:
        """å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†æµç¨‹"""
        text = TextPreprocessor.remove_brackets(text)
        text = TextPreprocessor.normalize_punctuation(text)
        text = TextPreprocessor.to_lowercase(text)
        text = TextPreprocessor.strip_whitespace(text)
        return text

class ProductNameExtractor:
    """äº§å“åæå–æ¨¡å—"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
    
    def extract_product_name(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–äº§å“åç§°"""
        prompt = PromptTemplate(
            input_variables=["text"],
            template="""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–äº§å“åç§°ã€‚äº§å“åç§°é€šå¸¸å‡ºç°åœ¨æ–¹æ‹¬å·ã€ã€‘ä¸­ï¼Œæˆ–è€…æ˜¯æ–‡æœ¬å¼€å¤´çš„ä¸»è¦äº§å“æ ‡è¯†ã€‚

æ–‡æœ¬ï¼š{text}

è¯·åªè¿”å›äº§å“åç§°ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°äº§å“åç§°ï¼Œè¿”å›"æœªè¯†åˆ«"ã€‚

äº§å“åç§°ï¼š
"""
        )
        
        try:
            response = self.llm.invoke(prompt.format(text=text))
            product_name = response.content.strip()
            
            # æ¸…ç†ç»“æœ
            if product_name == "æœªè¯†åˆ«" or not product_name:
                return ""
            
            # ç§»é™¤å¯èƒ½çš„å¼•å·æˆ–å…¶ä»–ç¬¦å·
            product_name = re.sub(r'["""]', '', product_name)
            return product_name
            
        except Exception as e:
            print(f"äº§å“åæå–å¤±è´¥: {e}")
            return ""

class DocumentUploader:
    """æ–‡æ¡£ä¸Šä¼ å¤„ç†å™¨"""
    
    def __init__(self):
        self.supported_formats = {
            '.pdf': PyPDFLoader,
            '.docx': Docx2txtLoader,
            '.doc': Docx2txtLoader,
            '.txt': TextLoader,
            '.md': TextLoader
        }
        self.image_processor = ImageProcessor()
    
    def load_document(self, file_path: str) -> List[Document]:
        """åŠ è½½æ–‡æ¡£æ–‡ä»¶"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶
        if self.image_processor.is_image_file(file_path):
            return self.load_image_document(file_path)
        
        if file_ext not in self.supported_formats:
            # å°è¯•ä½¿ç”¨é€šç”¨åŠ è½½å™¨
            loader = UnstructuredFileLoader(file_path)
        else:
            loader_class = self.supported_formats[file_ext]
            loader = loader_class(file_path)
        
        try:
            documents = loader.load()
            print(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path}, å…± {len(documents)} é¡µ")
            return documents
        except Exception as e:
            print(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def load_image_document(self, image_path: str) -> List[Document]:
        """åŠ è½½å›¾ç‰‡æ–‡æ¡£ï¼ˆOCRè¯†åˆ«ï¼‰"""
        try:
            # ä½¿ç”¨OCRæå–æ–‡å­—
            text = self.image_processor.extract_text_from_image(image_path)
            
            if not text:
                print(f"å›¾ç‰‡ {image_path} ä¸­æœªè¯†åˆ«åˆ°æ–‡å­—")
                return []
            
            # åˆ›å»ºDocumentå¯¹è±¡
            document = Document(
                page_content=text,
                metadata={"source": image_path, "type": "image"}
            )
            
            print(f"æˆåŠŸä»å›¾ç‰‡æå–æ–‡å­—: {image_path}")
            print(f"æå–çš„æ–‡å­—: {text[:100]}...")  # æ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            
            return [document]
            
        except Exception as e:
            print(f"å›¾ç‰‡OCRå¤„ç†å¤±è´¥: {e}")
            raise
    
    def load_multiple_documents(self, file_paths: List[str]) -> List[Document]:
        """æ‰¹é‡åŠ è½½å¤šä¸ªæ–‡æ¡£"""
        all_documents = []
        
        for file_path in file_paths:
            try:
                documents = self.load_document(file_path)
                all_documents.extend(documents)
            except Exception as e:
                print(f"è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
                continue
        
        return all_documents

class ComplianceKnowledgeBase:
    """åˆè§„çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, embeddings: OpenAIEmbeddings):
        self.embeddings = embeddings
        self.vectorstore = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.document_uploader = DocumentUploader()
        self.knowledge_base_path = "compliance_knowledge_base"
        
        # åˆè§„æŒ‡å¼•æ–‡æ¡£è·¯å¾„
        self.compliance_doc_path = "rules.docx"
    
    def build_knowledge_base_from_files(self, file_paths: List[str]):
        """ä»æ–‡ä»¶æ„å»ºçŸ¥è¯†åº“"""
        print("å¼€å§‹æ„å»ºåˆè§„çŸ¥è¯†åº“...")
        
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£
        documents = self.document_uploader.load_multiple_documents(file_paths)
        
        if not documents:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
        
        # åˆ†å‰²æ–‡æ¡£
        split_documents = self.text_splitter.split_documents(documents)
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        self.vectorstore = FAISS.from_documents(split_documents, self.embeddings)
        
        # ä¿å­˜çŸ¥è¯†åº“
        self.save_knowledge_base()
        
        print(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…± {len(split_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return len(split_documents)
    
    def build_knowledge_base_from_text(self, guide_text: str):
        """ä»æ–‡æœ¬æ„å»ºçŸ¥è¯†åº“"""
        print("ä»æ–‡æœ¬æ„å»ºåˆè§„çŸ¥è¯†åº“...")
        
        # å°†æŒ‡å¼•æ–‡æœ¬åˆ†å‰²æˆæ–‡æ¡£
        documents = self.text_splitter.create_documents([guide_text])
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        self.vectorstore = FAISS.from_documents(documents, self.embeddings)
        
        # ä¿å­˜çŸ¥è¯†åº“
        self.save_knowledge_base()
        
        print(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return len(documents)
    
    def load_compliance_document(self) -> str:
        """åŠ¨æ€åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£å†…å®¹"""
        if not os.path.exists(self.compliance_doc_path):
            print(f"è­¦å‘Šï¼šåˆè§„æŒ‡å¼•æ–‡æ¡£ {self.compliance_doc_path} ä¸å­˜åœ¨")
            return ""
        
        try:
            # ä½¿ç”¨DocumentUploaderåŠ è½½æ–‡æ¡£
            documents = self.document_uploader.load_document(self.compliance_doc_path)
            
            # åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹
            content = "\n".join([doc.page_content for doc in documents])
            print(f"æˆåŠŸåŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£ï¼š{self.compliance_doc_path}")
            return content
            
        except Exception as e:
            print(f"åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£å¤±è´¥ï¼š{e}")
            return ""
    
    def initialize_built_in_knowledge_base(self):
        """åˆå§‹åŒ–å†…ç½®çš„åˆè§„çŸ¥è¯†åº“"""
        print("åˆå§‹åŒ–Spesåˆè§„çŸ¥è¯†åº“...")
        
        # åŠ¨æ€åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£
        compliance_content = self.load_compliance_document()
        
        if not compliance_content:
            print("æ— æ³•åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™")
            # ä½¿ç”¨åŸºæœ¬çš„é»˜è®¤è§„åˆ™
            compliance_content = """
            Spesåˆè§„æŒ‡å¼•ï¼š
            
            ä¸€ã€æ ¸å¿ƒæ³•è§„ä¾æ®ä¸é€šç”¨ç¦ç”¨åŸåˆ™
            1. ç»å¯¹åŒ–è¯æ±‡ç¦ç”¨ï¼šç«‹ç«¿è§å½±ã€ç™¾åˆ†ä¹‹ç™¾ã€æ ¹æ²»ã€å®Œå…¨ã€ç»å¯¹ã€å½»åº•ç­‰
            2. åŒ»ç–—æœ¯è¯­ç¦ç”¨ï¼šä¿®å¤æ¯›å›Šã€æ²»ç–—ã€æ²»æ„ˆã€è¯ç‰©ã€å¤„æ–¹ç­‰
            3. è¶…èŒƒå›´å®£ä¼ ï¼šä¸å¾—è¶…å‡ºäº§å“å®é™…åŠŸæ•ˆèŒƒå›´
            
            äºŒã€äº§å“ä¸“å±ç¦ç”¨è¯æ±‡
            1. å¤šè‚½è“¬è“¬ç“¶-ä¿®æŠ¤ç±»äº§å“ï¼š
               - ç¦ç”¨è¯æ±‡ï¼š"ä¿®å¤æ¯›å›Š"ã€"ç”Ÿå‘"ã€"æ²»ç–—è„±å‘"
               - é£é™©ç­‰çº§ï¼šç»å¯¹ç¦æ­¢
               - è§„åˆ™å‡ºå¤„ï¼šæŒ‡å¼• 1ã€å¤šè‚½è“¬è“¬ç“¶-ä¿®æŠ¤
            """
        
        # ä½¿ç”¨åŠ è½½çš„åˆè§„è§„åˆ™æ„å»ºçŸ¥è¯†åº“
        documents = self.text_splitter.create_documents([compliance_content])
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        self.vectorstore = FAISS.from_documents(documents, self.embeddings)
        
        # ä¿å­˜çŸ¥è¯†åº“
        self.save_knowledge_base()
        
        print(f"åˆè§„çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return len(documents)
    
    def reload_compliance_document(self):
        """é‡æ–°åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£"""
        print("é‡æ–°åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£...")
        
        # é‡æ–°åŠ è½½æ–‡æ¡£å†…å®¹
        compliance_content = self.load_compliance_document()
        
        if not compliance_content:
            print("æ— æ³•åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£")
            return False
        
        # é‡æ–°æ„å»ºçŸ¥è¯†åº“
        documents = self.text_splitter.create_documents([compliance_content])
        
        # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
        self.vectorstore = FAISS.from_documents(documents, self.embeddings)
        
        # ä¿å­˜çŸ¥è¯†åº“
        self.save_knowledge_base()
        
        print(f"åˆè§„æŒ‡å¼•æ–‡æ¡£é‡æ–°åŠ è½½å®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return True
    
    def load_knowledge_base(self):
        """åŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†åº“"""
        if os.path.exists(self.knowledge_base_path):
            try:
                self.vectorstore = FAISS.load_local(
                    self.knowledge_base_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("æˆåŠŸåŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†åº“")
                return True
            except Exception as e:
                print(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
                return False
        return False
    
    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æœ¬åœ°"""
        if self.vectorstore:
            self.vectorstore.save_local(self.knowledge_base_path)
            print(f"çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {self.knowledge_base_path}")
    
    def search_compliance_rules(self, query: str, k: int = 5) -> List[Document]:
        """æœç´¢ç›¸å…³çš„åˆè§„è§„åˆ™"""
        if not self.vectorstore:
            print("çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½çŸ¥è¯†åº“")
            return []
        
        docs = self.vectorstore.similarity_search(query, k=k)
        return docs
    
    def get_knowledge_base_info(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ä¿¡æ¯"""
        if not self.vectorstore:
            return {"status": "æœªåˆå§‹åŒ–", "document_count": 0}
        
        try:
            # è·å–ç´¢å¼•ä¿¡æ¯
            index = self.vectorstore.index
            return {
                "status": "å·²åˆå§‹åŒ–",
                "document_count": index.ntotal if hasattr(index, 'ntotal') else "æœªçŸ¥",
                "dimension": index.d if hasattr(index, 'd') else "æœªçŸ¥"
            }
        except Exception as e:
            return {"status": "å·²åˆå§‹åŒ–", "error": str(e)}

class ComplianceMatcher:
    """åˆè§„åŒ¹é…å™¨"""
    
    def __init__(self, llm: ChatOpenAI, knowledge_base: ComplianceKnowledgeBase):
        self.llm = llm
        self.knowledge_base = knowledge_base
    
    def match_compliance_rules(self, text: str, product_name: str = "") -> List[ComplianceResult]:
        """åŒ¹é…åˆè§„è§„åˆ™"""
        results = []
        
        # æ„å»ºæŸ¥è¯¢
        query = f"äº§å“: {product_name}, æ–‡æœ¬: {text}"
        
        # æœç´¢ç›¸å…³è§„åˆ™
        relevant_docs = self.knowledge_base.search_compliance_rules(query, k=10)
        
        if not relevant_docs:
            return [ComplianceResult(
                category=product_name,
                original_text=text,
                review_result="å®‰å…¨é€šè¿‡"
            )]
        
        # ä½¿ç”¨LLMè¿›è¡Œåˆè§„åˆ†æ
        analysis_prompt = PromptTemplate(
            input_variables=["text", "product_name", "compliance_rules"],
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆè§„å®¡æŸ¥ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹åˆè§„è§„åˆ™åˆ†ææ–‡æœ¬çš„åˆè§„æ€§ã€‚

äº§å“åç§°: {product_name}
å¾…å®¡æŸ¥æ–‡æœ¬: {text}

åˆè§„è§„åˆ™:
{compliance_rules}

è¯·åˆ†ææ–‡æœ¬æ˜¯å¦è¿ååˆè§„è§„åˆ™ã€‚å¦‚æœè¿åï¼Œè¯·æä¾›è¯¦ç»†ä¿¡æ¯ï¼š
1. å‘½ä¸­çš„è¿è§„è¯æ±‡
2. é£é™©ç±»åˆ«ï¼ˆç»å¯¹åŒ–/åŒ»ç–—æœ¯è¯­/è¶…èŒƒå›´/äº§å“ä¸“å±ç¦ç”¨ï¼‰
3. é£é™©ç­‰çº§ï¼ˆç»å¯¹ç¦æ­¢/è­¦å‘Š/ç°è‰²æé†’ï¼‰
4. è§„åˆ™å‡ºå¤„
5. ç®€è¦è¯´æ˜

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "violations": [
        {{
            "hit_word": "è¿è§„è¯æ±‡",
            "risk_category": "é£é™©ç±»åˆ«",
            "risk_level": "é£é™©ç­‰çº§",
            "rule_source": "è§„åˆ™å‡ºå¤„",
            "brief_description": "ç®€è¦è¯´æ˜"
        }}
    ],
    "manual_review_needed": false
}}

å¦‚æœæ²¡æœ‰è¿è§„ï¼Œè¿”å›ï¼š
{{
    "violations": [],
    "manual_review_needed": false
}}
"""
        )
        
        # å‡†å¤‡åˆè§„è§„åˆ™æ–‡æœ¬
        rules_text = "\n".join([doc.page_content for doc in relevant_docs])
        
        try:
            response = self.llm.invoke(analysis_prompt.format(
                text=text,
                product_name=product_name,
                compliance_rules=rules_text
            ))
            
            # è§£æJSONå“åº”ï¼Œå¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
            content = response.content.strip()
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                content = content[json_start:json_end].strip()
            elif "```" in content:
                json_start = content.find("```") + 3
                json_end = content.find("```", json_start)
                content = content[json_start:json_end].strip()
            
            # å°è¯•è§£æJSON
            try:
                result_data = json.loads(content)
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£æ
                print(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•")
                result_data = self._parse_compliance_result_fallback(content, text, product_name)
            
            if result_data["violations"]:
                for violation in result_data["violations"]:
                    results.append(ComplianceResult(
                        category=product_name,
                        original_text=text,
                        review_result="æ‹’ç»",
                        hit_word=violation.get("hit_word", ""),
                        risk_category=violation.get("risk_category", ""),
                        risk_level=violation.get("risk_level", ""),
                        rule_source=violation.get("rule_source", ""),
                        brief_description=violation.get("brief_description", ""),
                        manual_review_needed=result_data.get("manual_review_needed", False)
                    ))
            else:
                results.append(ComplianceResult(
                    category=product_name,
                    original_text=text,
                    review_result="å®‰å…¨é€šè¿‡",
                    manual_review_needed=result_data.get("manual_review_needed", False)
                ))
                
        except Exception as e:
            print(f"åˆè§„åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å®‰å…¨é€šè¿‡ç»“æœ
            results.append(ComplianceResult(
                category=product_name,
                original_text=text,
                review_result="å®‰å…¨é€šè¿‡"
            ))
        
        return results
    
    def _parse_compliance_result_fallback(self, content: str, text: str, product_name: str) -> Dict[str, Any]:
        """å¤‡ç”¨è§£ææ–¹æ³•ï¼Œå½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨"""
        # ç®€å•çš„æ–‡æœ¬åˆ†æï¼Œæ£€æŸ¥å¸¸è§çš„è¿è§„è¯æ±‡
        violations = []
        
        # ç»å¯¹åŒ–è¯æ±‡æ£€æŸ¥
        absolute_words = ["æ ¹æ²»", "å½»åº•", "ç«‹ç«¿è§å½±", "ç™¾åˆ†ä¹‹ç™¾", "å®Œå…¨", "ç»å¯¹", "å…¨æ–¹ä½", "å…¨é¢", "é¡¶çº§", "æœ€", "ç¬¬ä¸€", "ç¬é—´", "æ°¸ä¸"]
        for word in absolute_words:
            if word in text:
                violations.append({
                    "hit_word": word,
                    "risk_category": "ç»å¯¹åŒ–",
                    "risk_level": "ç»å¯¹ç¦æ­¢",
                    "rule_source": "æŒ‡å¼• ä¸€ã€æ ¸å¿ƒæ³•è§„ä¾æ®ä¸é€šç”¨ç¦ç”¨åŸåˆ™",
                    "brief_description": f"ä½¿ç”¨ç»å¯¹åŒ–è¡¨è¿°: {word}"
                })
        
        # åŒ»ç–—æœ¯è¯­æ£€æŸ¥
        medical_words = ["æ¯›å›Š", "ä¿®å¤", "æ²»ç–—", "é™¤èŒ", "æŠ—èŒ", "æ’æ¯’", "æ´»åŒ–", "é¢ è¦†", "é€†è½¬", "è¯ç”¨", "æ¶ˆç‚", "æŠ—æ•"]
        for word in medical_words:
            if word in text:
                violations.append({
                    "hit_word": word,
                    "risk_category": "åŒ»ç–—æœ¯è¯­",
                    "risk_level": "ç»å¯¹ç¦æ­¢",
                    "rule_source": "æŒ‡å¼• ä¸€ã€æ ¸å¿ƒæ³•è§„ä¾æ®ä¸é€šç”¨ç¦ç”¨åŸåˆ™",
                    "brief_description": f"ä½¿ç”¨åŒ»ç–—æœ¯è¯­: {word}"
                })
        
        return {
            "violations": violations,
            "manual_review_needed": False
        }

class ComplianceAgent:
    """åˆè§„å®¡æŸ¥Agentä¸»ç±»"""
    
    def __init__(self):
        # åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹
        self.llm = ChatOpenAI(model="gpt-4o-mini")
        self.embeddings = OpenAIEmbeddings()
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.preprocessor = TextPreprocessor()
        self.product_extractor = ProductNameExtractor(self.llm)
        self.knowledge_base = ComplianceKnowledgeBase(self.embeddings)
        self.matcher = ComplianceMatcher(self.llm, self.knowledge_base)
        
        # è‡ªåŠ¨åˆå§‹åŒ–çŸ¥è¯†åº“
        self._initialize_knowledge_base()
        
        # åˆå§‹åŒ–Agentå·¥å…·
        self.tools = self._create_tools()
        self.agent_executor = self._create_agent()
    
    def _initialize_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        # é¦–å…ˆå°è¯•åŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†åº“
        if not self.knowledge_base.load_knowledge_base():
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ™åˆå§‹åŒ–å†…ç½®çŸ¥è¯†åº“
            print("æœªæ‰¾åˆ°å·²ä¿å­˜çš„çŸ¥è¯†åº“ï¼Œæ­£åœ¨åˆå§‹åŒ–å†…ç½®Spesåˆè§„çŸ¥è¯†åº“...")
            self.knowledge_base.initialize_built_in_knowledge_base()
        else:
            print("æˆåŠŸåŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†åº“")
    
    def _create_tools(self):
        """åˆ›å»ºAgentå·¥å…·"""
        
        
        @tool
        def smart_review(text: str = "", image_path: str = "") -> str:
            """
            æ™ºèƒ½åˆè§„å®¡æŸ¥ï¼šæ”¯æŒæ–‡æœ¬+å›¾ç‰‡ç»„åˆè¾“å…¥
            
            Args:
                text: æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼‰
                image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
            Returns:
                å®¡æŸ¥ç»“æœï¼Œä»¥è¡¨æ ¼æ ¼å¼è¾“å‡º
            """
            return self.review_with_image(text, image_path)
        
        @tool
        def get_knowledge_base_status() -> str:
            """
            è·å–çŸ¥è¯†åº“çŠ¶æ€ä¿¡æ¯
            
            Returns:
                çŸ¥è¯†åº“çŠ¶æ€ä¿¡æ¯
            """
            info = self.knowledge_base.get_knowledge_base_info()
            return f"çŸ¥è¯†åº“çŠ¶æ€: {info['status']}, æ–‡æ¡£æ•°é‡: {info.get('document_count', 'æœªçŸ¥')}"
        
        @tool
        def add_custom_rules(custom_text: str) -> str:
            """
            æ·»åŠ è‡ªå®šä¹‰åˆè§„è§„åˆ™åˆ°çŸ¥è¯†åº“
            
            Args:
                custom_text: è‡ªå®šä¹‰åˆè§„è§„åˆ™æ–‡æœ¬
            
            Returns:
                æ·»åŠ ç»“æœä¿¡æ¯
            """
            try:
                # å°†è‡ªå®šä¹‰è§„åˆ™æ·»åŠ åˆ°ç°æœ‰çŸ¥è¯†åº“
                documents = self.knowledge_base.text_splitter.create_documents([custom_text])
                
                if self.knowledge_base.vectorstore:
                    # æ·»åŠ åˆ°ç°æœ‰å‘é‡æ•°æ®åº“
                    self.knowledge_base.vectorstore.add_documents(documents)
                    self.knowledge_base.save_knowledge_base()
                    return f"æˆåŠŸæ·»åŠ è‡ªå®šä¹‰è§„åˆ™ï¼Œæ–°å¢ {len(documents)} ä¸ªçŸ¥è¯†ç‰‡æ®µ"
                else:
                    return "çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ è‡ªå®šä¹‰è§„åˆ™"
            except Exception as e:
                return f"æ·»åŠ è‡ªå®šä¹‰è§„åˆ™å¤±è´¥: {str(e)}"
        
        @tool
        def reload_compliance_document() -> str:
            """
            é‡æ–°åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£
            
            Returns:
                é‡æ–°åŠ è½½ç»“æœä¿¡æ¯
            """
            try:
                success = self.knowledge_base.reload_compliance_document()
                if success:
                    return "åˆè§„æŒ‡å¼•æ–‡æ¡£é‡æ–°åŠ è½½æˆåŠŸ"
                else:
                    return "åˆè§„æŒ‡å¼•æ–‡æ¡£é‡æ–°åŠ è½½å¤±è´¥"
            except Exception as e:
                return f"é‡æ–°åŠ è½½å¤±è´¥: {str(e)}"
        
        return [smart_review, get_knowledge_base_status, add_custom_rules, reload_compliance_document]
    
    def _create_agent(self):
        """åˆ›å»ºAgentæ‰§è¡Œå™¨"""
        # è·å–promptæ¨¡æ¿
        prompt = hub.pull("hwchase17/openai-functions-agent")
        
        # åˆ›å»ºAgent
        agent = create_tool_calling_agent(self.llm, self.tools, prompt)
        
        # åˆ›å»ºæ‰§è¡Œå™¨
        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)
        
        return agent_executor
    
    def _perform_compliance_review(self, text: str) -> str:
        """æ‰§è¡Œåˆè§„å®¡æŸ¥"""
        try:
            # 1. æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å·²åˆå§‹åŒ–
            if not self.knowledge_base.vectorstore:
                # å°è¯•åŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†åº“
                if not self.knowledge_base.load_knowledge_base():
                    return "é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆä¸Šä¼ åˆè§„æŒ‡å¼•æ–‡æ¡£"
            
            # 2. æ–‡æœ¬é¢„å¤„ç†
            processed_text = self.preprocessor.preprocess(text)
            
            # 3. æå–äº§å“åç§°
            product_name = self.product_extractor.extract_product_name(processed_text)
            
            # 4. åˆè§„åŒ¹é…
            results = self.matcher.match_compliance_rules(processed_text, product_name)
            
            # 5. æ ¼å¼åŒ–è¾“å‡º
            return self._format_output(results)
            
        except Exception as e:
            return f"å®¡æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def _format_output(self, results: List[ComplianceResult]) -> str:
        """æ ¼å¼åŒ–è¾“å‡ºç»“æœ"""
        if not results:
            return "| å“ç±» | åŸæ–‡è¾“å…¥ | å®¡æ ¸ç»“æœ |\n| ------ | ------ | ---- |\n|  |  | å®‰å…¨é€šè¿‡ |"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿è§„
        has_violations = any(result.review_result == "æ‹’ç»" for result in results)
        
        if has_violations:
            # æœ‰è¿è§„çš„æƒ…å†µ
            output = "| å“ç±» | åŸæ–‡è¾“å…¥ | å®¡æ ¸ç»“æœ | å‘½ä¸­è¯ | é£é™©ç±»åˆ« | é£é™©ç­‰çº§ | è§„åˆ™å‡ºå¤„ | ç®€è¦è¯´æ˜ |\n"
            output += "| ------ | ------ | ---- | ------- | ------ | ------ | -------- | ----------- |\n"
            
            for result in results:
                if result.review_result == "æ‹’ç»":
                    output += f"| {result.category} | {result.original_text} | {result.review_result} | {result.hit_word} | {result.risk_category} | {result.risk_level} | {result.rule_source} | {result.brief_description} |\n"
        else:
            # å®‰å…¨é€šè¿‡çš„æƒ…å†µ
            output = "| å“ç±» | åŸæ–‡è¾“å…¥ | å®¡æ ¸ç»“æœ |\n"
            output += "| ------ | ------ | ---- |\n"
            
            for result in results:
                output += f"| {result.category} | {result.original_text} | {result.review_result} |\n"
        
        return output
    
    def upload_documents(self, file_paths: List[str]) -> str:
        """ä¸Šä¼ åˆè§„æŒ‡å¼•æ–‡æ¡£"""
        try:
            doc_count = self.knowledge_base.build_knowledge_base_from_files(file_paths)
            return f"æˆåŠŸä¸Šä¼  {len(file_paths)} ä¸ªæ–‡æ¡£ï¼Œæ„å»ºäº† {doc_count} ä¸ªçŸ¥è¯†ç‰‡æ®µ"
        except Exception as e:
            return f"ä¸Šä¼ å¤±è´¥: {str(e)}"
    
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return self.knowledge_base.get_knowledge_base_info()
    
    def reload_document(self) -> str:
        """é‡æ–°åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£"""
        try:
            success = self.knowledge_base.reload_compliance_document()
            if success:
                return "åˆè§„æŒ‡å¼•æ–‡æ¡£é‡æ–°åŠ è½½æˆåŠŸ"
            else:
                return "åˆè§„æŒ‡å¼•æ–‡æ¡£é‡æ–°åŠ è½½å¤±è´¥"
        except Exception as e:
            return f"é‡æ–°åŠ è½½å¤±è´¥: {str(e)}"
    
    
    def review_with_image(self, text: str = "", image_path: str = "") -> str:
        """æ™ºèƒ½å®¡æŸ¥ï¼šæ”¯æŒæ–‡æœ¬+å›¾ç‰‡ç»„åˆè¾“å…¥"""
        try:
            # æ„å»ºå®Œæ•´çš„è¾“å…¥æ–‡æœ¬
            full_text = ""
            
            # å¤„ç†æ–‡æœ¬éƒ¨åˆ†
            if text and text.strip():
                full_text = text.strip()
            
            # å¤„ç†å›¾ç‰‡éƒ¨åˆ†
            if image_path and os.path.exists(image_path):
                if self.knowledge_base.document_uploader.image_processor.is_image_file(image_path):
                    print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾ç‰‡è¾“å…¥: {image_path}")
                    image_text = self.knowledge_base.document_uploader.image_processor.extract_text_from_image(image_path)
                    
                    if image_text:
                        if full_text:
                            # æ–‡æœ¬å’Œå›¾ç‰‡éƒ½æœ‰ï¼Œåˆå¹¶
                            full_text = f"{full_text} {image_text}"
                            print(f"ğŸ“ åˆå¹¶è¾“å…¥: {full_text}")
                        else:
                            # åªæœ‰å›¾ç‰‡
                            full_text = image_text
                            print(f"ğŸ“ å›¾ç‰‡æ–‡å­—: {full_text}")
                    else:
                        return f"å›¾ç‰‡ {image_path} ä¸­æœªè¯†åˆ«åˆ°æ–‡å­—"
                else:
                    return f"é”™è¯¯ï¼šä¸æ˜¯æ”¯æŒçš„å›¾ç‰‡æ ¼å¼: {image_path}"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆè¾“å…¥
            if not full_text.strip():
                return "é”™è¯¯ï¼šæ²¡æœ‰æä¾›æœ‰æ•ˆçš„æ–‡æœ¬æˆ–å›¾ç‰‡è¾“å…¥"
            
            # æ‰§è¡Œåˆè§„å®¡æŸ¥
            result = self._perform_compliance_review(full_text)
            
            # æ·»åŠ è¾“å…¥ä¿¡æ¯
            input_info = ""
            if text and text.strip():
                input_info += f"æ–‡æœ¬è¾“å…¥: {text}\n"
            if image_path and os.path.exists(image_path):
                input_info += f"å›¾ç‰‡è¾“å…¥: {image_path}\n"
            input_info += f"å®Œæ•´è¾“å…¥: {full_text}\n\n"
            
            return input_info + result
            
        except Exception as e:
            return f"æ™ºèƒ½å®¡æŸ¥å¤±è´¥: {str(e)}"

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    # åˆ›å»ºåˆè§„å®¡æŸ¥Agentï¼ˆè‡ªåŠ¨åˆå§‹åŒ–å†…ç½®çŸ¥è¯†åº“ï¼‰
    print("æ­£åœ¨åˆå§‹åŒ–Spesåˆè§„å®¡æŸ¥Agent...")
    agent = ComplianceAgent()
    
    # ç¤ºä¾‹æ–‡æœ¬
    sample_texts = [
        "ã€å¹²å‘å–·é›¾ã€‘",  # ä½ çš„æ–‡æœ¬
        "ã€å¤šè‚½è“¬è“¬ç“¶ã€‘æœ¬äº§å“èƒ½æ ¹æ²»è„±å‘å¹¶ä¿®å¤æ¯›å›Šï¼Œæ•ˆæœç«‹ç«¿è§å½±",
        "ã€æ´—å‘æ°´ã€‘æ¸©å’Œæ¸…æ´ï¼Œå‘µæŠ¤ç§€å‘å¥åº·",
        "ã€é¢è†œã€‘æ·±å±‚è¡¥æ°´ï¼Œè®©è‚Œè‚¤æ°´æ¶¦å…‰æ»‘",
        "ã€æŠ¤å‘ç´ ã€‘ç™¾åˆ†ä¹‹ç™¾æœ‰æ•ˆï¼Œå½»åº•è§£å†³å¤´å‘é—®é¢˜"
    ]
    
    # ä½ çš„å›¾ç‰‡è·¯å¾„
    your_image_path = r"C:\Users\1\Desktop\æ°´æ¨é…¸æ´—å‘æ°´è¯¦æƒ…é¡µ_05.jpg"
    
    print("\n" + "=" * 80)
    print("Spesåˆè§„å®¡æŸ¥Agentæ¼”ç¤º")
    print("=" * 80)
    
    # æ‰§è¡Œå®¡æŸ¥
    for i, text in enumerate(sample_texts, 1):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}:")
        print(f"è¾“å…¥æ–‡æœ¬: {text}")
        print("-" * 50)
        
        result = agent.review(text)
        print("å®¡æŸ¥ç»“æœ:")
        print(result)
        print("-" * 50)
    
    # å›¾ç‰‡å®¡æŸ¥æ¼”ç¤º
    print(f"\nğŸ–¼ï¸ å›¾ç‰‡å®¡æŸ¥æ¼”ç¤º:")
    print(f"å›¾ç‰‡è·¯å¾„: {your_image_path}")
    print("-" * 50)
    
    try:
        image_result = agent.review_image(your_image_path)
        print("å›¾ç‰‡å®¡æŸ¥ç»“æœ:")
        print(image_result)
    except Exception as e:
        print(f"å›¾ç‰‡å®¡æŸ¥å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("2. å›¾ç‰‡è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("3. æ˜¯å¦å®‰è£…äº†Tesseract OCR")
    
    print("-" * 50)
    
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    print("\nç³»ç»ŸçŠ¶æ€ï¼š")
    status = agent.get_status()
    print(f"çŸ¥è¯†åº“çŠ¶æ€: {status}")
    
    print("\n" + "=" * 80)
    print("Agentå·²å‡†å¤‡å°±ç»ªï¼ç°åœ¨ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ï¼š")
    print("1. agent.review('ä½ çš„æ–‡æœ¬') - ç›´æ¥å®¡æŸ¥æ–‡æœ¬")
    print("2. agent.review_image('å›¾ç‰‡è·¯å¾„') - ç›´æ¥å®¡æŸ¥å›¾ç‰‡")
    print("3. agent.agent_executor.invoke({'input': 'è¯·å®¡æŸ¥æ–‡æœ¬: xxx'}) - Agentå¯¹è¯æ–¹å¼")
    print("4. agent.agent_executor.invoke({'input': 'è¯·å®¡æŸ¥å›¾ç‰‡: å›¾ç‰‡è·¯å¾„'}) - Agentå›¾ç‰‡å®¡æŸ¥")
    print("5. agent.add_custom_rules('è‡ªå®šä¹‰è§„åˆ™') - æ·»åŠ è‡ªå®šä¹‰è§„åˆ™")
    print("6. agent.reload_document() - é‡æ–°åŠ è½½åˆè§„æŒ‡å¼•æ–‡æ¡£")
    print("7. agent.get_status() - æŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€")
    print("\nğŸ’¡ æç¤ºï¼š")
    print("   - ä¿®æ”¹ rules.docx æ–‡æ¡£åï¼Œè°ƒç”¨ agent.reload_document() å³å¯æ›´æ–°çŸ¥è¯†åº“")
    print("   - æ”¯æŒçš„å›¾ç‰‡æ ¼å¼ï¼šjpg, jpeg, png, bmp, gif, tiff, webp")
    print("   - å›¾ç‰‡ä¼šè‡ªåŠ¨è¿›è¡ŒOCRæ–‡å­—è¯†åˆ«ï¼Œç„¶åè¿›è¡Œåˆè§„å®¡æŸ¥")
    print("=" * 80)

if __name__ == "__main__":
    main()
